{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What can you do when you need a fast, compact, yet highly accurate model?\n",
    "- to speed up the predictions\n",
    "- to reduce the memory footprint\n",
    "\n",
    "HOW :\n",
    "- knowledge distillation\n",
    "- quantization\n",
    "- pruning\n",
    "- graph optimization\n",
    "- with Open Neural Network Exchange (ONNX) format & ONNX Runtime (ORT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intent Detectinon as a Case Study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "만약 어떤 회사의 콜센터에서 고객이 human agent와의 상호작용 없이 예약을 하려면,\n",
    "\n",
    "고객의 요구가 무엇인지 알아야 함.\n",
    "\n",
    "- need to be able to handle out-of-scope queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BERT-base model perform with 96% accuracy on the CLINC150 dataset.\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "bert_ckpt = \"transformersbook/bert-base-uncased-finetuned-clinic\"\n",
    "pipe = pipeline(\"text-classification\", model= bert_ckpt)\n",
    "# 이제 pipeline이 있으니까 we can pass query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Hey, I'd like to rent a vehicle from Nov 1st to Nov 15th in Paris and I need a 15 passenger van\"\n",
    "pipe(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Performance Benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Performance\n",
    "- : How well does our model perform in a well-crafted test set that reflects production data?\n",
    "    - especially important when cost of making errors is large\n",
    "    - and best mitigated(완화시키다) with a human in loop\n",
    "\n",
    "Lantency(지연 시간)\n",
    "- : How fast our model deliver predictions?\n",
    "    - usually care in real-time environments that deal with a lot of traffic \n",
    "    - like Stack Overflow\n",
    "        - needed a classifier to quickly detect unwelcome comments on the website\n",
    "\n",
    "Memory\n",
    "- : How can we deploy billion-parameter models like GPT-2 or T5 that require giga-bytes of disk storage and RAM?\n",
    "    - especially important when model has to generate predictions without access to a powerful cloud server\n",
    "\n",
    "BUT\n",
    "- More commonly, these can lead to balooning costs from running expensive cloud servers that may only need to handle a few requests\n",
    "- HOW each of these constraints can be optimized???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerformanceBenchmark:\n",
    "    # define optim_type\n",
    "    def __init__(self, pipeline, dataset, optim_type=\"BERT baseline\"):\n",
    "        self.pipeline = pipeline\n",
    "        self.dataset = dataset\n",
    "        self.optim_type = optim_type\n",
    "    \n",
    "    def compute_accuracy(self):\n",
    "        # We will define this later\n",
    "        pass\n",
    "    \n",
    "    def compute_size(self):\n",
    "        # We will define this later\n",
    "        pass\n",
    "    \n",
    "    def time_pipeline(self):\n",
    "        # We will define this later\n",
    "        pass\n",
    "    \n",
    "    def run_benchmark(self):\n",
    "        metrics = {}\n",
    "        metrics[self.optim_type] = self.compute_size()\n",
    "        metrics[self.optim_type].update(self.time_pipeline())\n",
    "        metrics[self.optim_type].update(self.compute_accuracy())\n",
    "        return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define \"optim_type\" parameter\n",
    "- to keep track of the differnet optimization techique\n",
    "\n",
    "run_benchmark() method\n",
    "- to collect all the metrics in a dictionary with keys given by \"optim_type\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CLINC150 dataset (used to finetune our baseline models)\n",
    "from datasets import load_dataset \n",
    "\n",
    "clinc = load_dataset(\"clinc_oos\", \"plus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = clinc[\"test\"][42]\n",
    "sample\n",
    "\n",
    "# intents are provided as IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intents = clinc[\"test\"].features[\"intent\"]\n",
    "intents.int2str(sample[\"intent\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "accuracy_score = load_metric(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (2735101485.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[4], line 2\u001b[1;36m\u001b[0m\n\u001b[1;33m    \u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "def compute_accuracy(self):\n",
    "    \"\"\"This overrides the PerformanceBenchmark.compute_accuracy() method\"\"\"\n",
    "    preds, labels = [], []\n",
    "    for example in self.dataset:\n",
    "        pred = self.pipeline(example[\"text\"][0][\"label\"])\n",
    "        label = example[\"intent\"]\n",
    "        preds.append(intents.str2int(pred)) # map each prediction to its corresponding IDs\n",
    "        labels.append(label)\n",
    "    \n",
    "    accuracy = accuracy_score.compute(predictions=preds, references=labels)\n",
    "    print(f\"Accuracy on test set - {accuracy['accuracy']: .3f}\")\n",
    "    return accuracy\n",
    "\n",
    "PerformanceBenchmark.compute_accuracy = compute_accuracy # add to class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(pipe.model.state_dict().items())[42] \n",
    "# We can clearly see that each key-value pair corresponding to specific layer and tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.save(pipe.model.state_dict(), \"model.pt\") #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pathlib import Path # to get info about the underlying files\n",
    "\n",
    "def compute_size(self):\n",
    "    \n",
    "    state_dict = self.pipeline.model.state_dict()\n",
    "    tmp_path = Path(\"model.pt\")\n",
    "    torch.save(state_dict, tmp_path)\n",
    "    # Calculate size in megabytes\n",
    "    size_mb = Path(tmp_path).stat().st_size / (1024 * 1024) # five size of model in bytes\n",
    "    # Delete temporary file\n",
    "    tmp_path.unlink()\n",
    "    print(f\"Model size (MB) - {size_mb: .2f}\")\n",
    "    return {\"size_mb\" : size_mb}\n",
    "\n",
    "PerformanceBenchmark.compute_size = compute_size # add to class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import perf_counter # by passing our test query & calculating time difference\n",
    "\n",
    "for _ in range(3):\n",
    "    start_time = perf_counter()\n",
    "    _ = pipe(query)\n",
    "    latency = perf_counter() - start_time\n",
    "    print(f\"Latency (ms) - {1000 * latency: .3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "def time_pipeline(self, query=\"What is the pin number for my account?\"):\n",
    "    \"\"\"This overrides the PerformanceBenchmark.time_pipeline() method\"\"\"\n",
    "    latencies = []\n",
    "    #Warm up\n",
    "    for _ in range(10):\n",
    "        _ = self.pipeline(query)\n",
    "    # Timed run\n",
    "    for _ in range(100):\n",
    "        _ = self.pipeline(query)\n",
    "        latency = perf_counter() - start_time\n",
    "        latencies.append(latency)\n",
    "        \n",
    "    # Compute run statistics\n",
    "    time_avg_ms = 1000 * np.mean(latencies)\n",
    "    time_std_ms = 1000 * np.std(latencies)\n",
    "    print(f\"Average latency (ms) - {time_avg_ms: .2f} +\\- {time_std_ms: .2f}\")\n",
    "    return {\"time_avg_ms\": time_avg_ms, \"time_std_ms\": time_std_ms}\n",
    "\n",
    "PerformanceBenchmark.time_pipeline = time_pipeline        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
